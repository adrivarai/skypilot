envs:
  MODEL_NAME: NousResearch/Meta-Llama-3-8B-Instruct

resources:
  accelerators: A100-80GB:8

setup: |
  uv venv --python 3.10 --seed
  source .venv/bin/activate

  # Install fschat and accelerate for chat completion
  git clone https://github.com/vllm-project/vllm.git || true
  uv pip install "vllm>=0.8.3"



run: |
  echo 'Starting vllm api server...'
  vllm serve $MODEL_NAME \
    --port 8081 \
    --dtype auto \
    --tensor-parallel-size $SKYPILOT_NUM_GPUS_PER_NODE 2>&1 | tee api_server.log &

  echo 'Waiting for vllm api server to start...'
  while ! `cat api_server.log | grep -q 'Application startup complete'`; do sleep 1; done

  echo 'Starting gradio server...'
  python vllm/examples/gradio_webserver.py
